{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "from deepface import DeepFace\n",
    "from scipy.spatial.distance import cosine\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "batch_size = 8\n",
    "images_path = glob.glob(\"../my_images/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(target_size[0], target_size[1], 3))\n",
    "\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def extract_feature(img_path):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    features = feature_extractor.predict(img_array)\n",
    "    features = features.flatten()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = []\n",
    "for img_path in images_path:\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    extracted_vector = extract_feature(img_path).flatten()\n",
    "    image_features.append(extracted_vector)\n",
    "\n",
    "image_features = np.array(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(image_features.shape[1])\n",
    "index.add(image_features)\n",
    "\n",
    "def find_closest(img_path):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    extracted_vector = feature_extractor.predict(img_array).flatten()\n",
    "    extracted_vector = np.array([extracted_vector])\n",
    "    _, result_search = index.search(extracted_vector, 1)\n",
    "\n",
    "    return images_path[result_search[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_it_me(img_path):\n",
    "    extracted_vector = extract_feature(img_path)\n",
    "    for my_img_vector in image_features:\n",
    "        similarity = 1 - cosine(my_img_vector, extracted_vector)\n",
    "        print(similarity)\n",
    "        if similarity > 0.7:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "print(\"This is you Abolfazl!\") if is_it_me(\"../test_images/test.jpg\") else print(\"This is not you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for my_img in images_path:\n",
    "    try:\n",
    "        embedding = DeepFace.represent(img_path=my_img, model_name=\"Facenet\")\n",
    "    except:\n",
    "        print(\"Face has not detected!\")\n",
    "    embeddings.append(embedding[0][\"embedding\"])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_it_me(img):\n",
    "    embedding = DeepFace.represent(\"../test_images/test.jpg\", model_name=\"Facenet\")\n",
    "    embedding_vector = embedding[0][\"embedding\"]\n",
    "    fa = embedding[0][\"facial_area\"]\n",
    "    x, y, w, h = fa[\"x\"], fa[\"y\"], fa[\"w\"], fa[\"h\"]\n",
    "    for em in embeddings:\n",
    "        similarity = 1 - cosine(em, embedding_vector)\n",
    "        if similarity > 0.7:\n",
    "            return embedding[0], True, (x, y, w, h)\n",
    "    return None, False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"Abolfazl Detector\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"Abolfazl Detector\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 13:\n",
    "        embedding, result, face_info = is_it_me(frame)\n",
    "\n",
    "        if result:\n",
    "            x, y, w, h = face_info\n",
    "\n",
    "            text = \"This is you, Abolfazl!\" if result else print(\"This is not you.\")\n",
    "            color = (0, 255, 0) if result else (0, 0, 255)\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "            cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                                0.8, color, 2, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.imshow(\"Abolfazl Detector\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
